{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 5922 Final Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Practice Interview Questions\n",
    "\n",
    "1)\tWhat is an activation function (in neural networks) and why is it used?  \n",
    "\n",
    "An activation function is a mathematical operation that is applied to the output of a neuron in a neural network to generate an “activated” output. Because neurons are typically a linear sum-product of weights and input values, they have difficulty approximating non-linear functions, such as the infamous XOR problem. Activation functions are typically non-linear, such as sigmoids or hyberbolic tangents, which give the network the ability to approximate non-linear functions. \n",
    "\n",
    "2)\tWhat is an exploding gradient and give an example of what could cause it.\n",
    "\n",
    "An exploding gradient is a quickly increasing derivative of the loss function with respect to the model parameters. The gradient is used to determine the direction and size of the change of model parameters when performing backpropagation, in tandem with the learning rate. If the gradient is increasing too much, the model will not converge to a minimum. This could occur easily in a recurrent neural network that has long series of inputs because the gradient for some parameters is a function of all prior inputs. If the values in this chain are > 1, their product and thus the gradient can get very large very quickly. \n",
    "\n",
    "3)\tWhen using an activation function in a CNN that predicts images, why might you choose the ReLU?\n",
    "\n",
    "ReLU’s power is in the simplicity of its calculation and of its derivative—for values below 0, the derivative is 0, and for values above 0, the derivate is 1. Furthermore, any negative values are simply reduced to a 0. This is important in a CNN designed to work with images because the input typically has really high dimensionality and thus the model has many layers with many parameters. ReLU helps to keep the derivatives and thus the gradient within reasonable values. \n",
    "\n",
    "4)\tA transformer (such as for language translation) has an encoder and a decoder. Suppose you have a word that is one-hot encoded. What 4 things will a common encoder include when embedding that word? Hint: The first one is some kind of embedding. What are the other three?\n",
    "\n",
    "A transformer encoder commonly includes first the embedding of the word into a vector space that captures the meaning of the word. Next, positional encoding is added to capture the location of the word in the input. Next, a self-attention mechanism captures the relative importance of the word in relation to the other words in the input. Finally, normalization and feed forward layers encode all of the above, typically into other encoder blocks.\n",
    "\n",
    "5)\tDefine BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained natural language processing model developed by Google. It utilizes a transformer architecture and is trained on vast amounts of text data to learn contextualized representations of words. BERT is bidirectional, meaning it considers both left and right context in a sentence, enabling it to capture richer semantic meanings. It has achieved state-of-the-art results in various NLP tasks by fine-tuning its pre-trained weights on specific downstream tasks.\n",
    "\n",
    "6)\tWhat is cross attention in a transformer?\n",
    "\n",
    "Cross-attention is a mechanism by which a transformer model pays varying degrees of attention to different parts of the input sequence when generating the output sequence. Unlike self-attention, which considers relationships within a single sequence, cross-attention involves attending to positions in the input sequence based on the context provided by another sequence.\n",
    "\n",
    "7)\tWhat is transfer learning?\n",
    "\n",
    "Transfer learning is a machine learning model training technique that involves pre-training the model on a different but similar dataset to the target task before fine-tuning on the specific task at hand. It is a powerful technique that allows a model to learn on a vast trove of labeled or unlabeled data before attempting a task that might have much less data. \n",
    "\n",
    "8)\tDefine GAN (Generative adversarial network)?\n",
    "\n",
    "A GAN is a type of neural network model that learns from the competing aims of a generator and a discrimator. The discriminator is attempting to learn to distinguish between a real input and a fake one, while the generator simultaneously learns to create fake inputs that are indistinguishable from real ones. They achieved impressive results in the field of generative AI. \n",
    "\n",
    "9)\tDefine GPT.\n",
    "\n",
    "GPT stands for Generative Pre-Trained Transformer. It refers to the family of language models based on transformers that are pre-trained on massive amounts of diverse text data in order to learn contextualized representations of words. The generative part refers to the fact that after these models are prompted with input, they generate a bunch of coherent and contextually relevant text. \n",
    "\n",
    "10)\tDefine ChatGPT.\n",
    "\n",
    "ChatGPT is a software product created by OpenAI in late 2022 that brought a lot of joy to students and a lot of headaches for teachers accustomed to assigning boring, rote assignments. Why? Well, based on the GPT defined above, OpenAI made it possible for anyone with an internet connection to prompt their large language model GPT3.5 and get a coherent response. ChatGPT has since passed the bar exam, med school exams, and written likely millions of high school English essays across the world.  If ChatGPT can answer this question, does that make it self aware?\n",
    "\n",
    "11)\tCommon activation functions include ReLU, sigmoid, tanh, and softmax (among others). Give an example when you would use the sigmoid as the last activation function in a NN. Give an example when you would use softmax as the last activation function in a NN. \n",
    "\n",
    "Sigmoid is a very useful activation function that is excellent for binary classification. Because it’s shape, it normalizes the input value to between 0 and 1 and for most real numbers, the value is very close to 1 or 0. The softmax function is similarly useful for classification, but instead for multiclass problems. It takes a whole range of input weights and normalizes them such that they are all between 0 and 1 and add up to one, making the outputs interpretable as probabilities that the input belongs to a given class.\n",
    "\n",
    "12)\tSuppose you have labeled input data where the labels are one-hot encoded. Suppose also that your labels can be one of three categories (like dog, cat, mouse for example). Next, suppose the last activation function of your NN is the softmax. Which Loss function would you choose to use in this case and why?\t\n",
    "\n",
    "For this case, I would most certainly use categorical cross entropy because this is a classification problem with more than 2 classes. CCE rewards a model not just for guessing the correct category, but it also rewards guessing correctly with high confidence. It therefore performs excellently at training the model progressively, helping the model get slowly more and more confident as it trains. \n",
    "\n",
    "13)\tWhy use max pooling CNNs – what does max pooling do?\n",
    "\n",
    "Max pooling is a type of layer whose primary purpose is to reduce the dimensionality of the features in a model.  It does this by defining a pool, or patch, of a given size (say 3 by 3) and pulling out the maximum value from within that pool. This simplifies the input (in our case 9 values) down to just a single value that still captures a lot of the important signal. It’s commonly used with images which have notoriously high dimensionality to get us from millions of pixels down to feature space of just a few thousand categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ANN & CNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1A: ANN Diagram\n",
    "\n",
    "![ANN Diagram](ann_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1B: ANN Derivatives  \n",
    "  \n",
    "#### Equations for ANN:  \n",
    "Z1 = X * W1 + B  \n",
    "H1 = σ(Z1)  \n",
    "Z2 = H1 * W2 + C  \n",
    "H2 = ReLU(Z2)  \n",
    "ŷ = softmax(H2)  \n",
    "L = CCE(y, ŷ)  \n",
    "L = -y * log(ŷ)  \n",
    "  \n",
    "#### Derviative ∂L/∂W2_11:\n",
    "  \n",
    "∂L/∂W2_11 = ∂L/∂ŷ * ∂ŷ/∂H2 * ∂H2/∂Z2 * ∂Z2/∂W2_11  \n",
    "  \n",
    "∂L/∂ŷ = -y/ŷ  \n",
    "∂ŷ/∂H2 = ŷ * (1 - ŷ)  \n",
    "∂H2/∂Z2 = 1 if Z2 > 0 else 0  \n",
    "∂Z2/∂W2_11 = H1  \n",
    "  \n",
    "if Z2 > 0:  \n",
    "∂L/∂W2_11 = -y/ŷ * ŷ * (1 - ŷ) * 1 * H1  \n",
    "∂L/∂W2_11 = -y * (1 - ŷ) * H1  \n",
    "  \n",
    "if Z2 <= 0:  \n",
    "∂L/∂W2_11 = -y/ŷ * ŷ * (1 - ŷ) * 0 * H1  \n",
    "∂L/∂W2_11 = 0  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 15        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47\n",
      "Trainable params: 47\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create this ANN in Keras\n",
    "\n",
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2A: CNN Diagram\n",
    "\n",
    "![CNN Diagram](cnn_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 2)         20        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 2)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 15, 15, 4)         76        \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 4)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 196)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                12608     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,899\n",
      "Trainable params: 12,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the CNN in Keras\n",
    "\n",
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# create the model\n",
    "cnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=2, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(30, 30, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=4, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=3, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "cnn.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# print the model summary\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Applying Neural Nets (ANN, CNN, LSTM) on Real Labeled Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
