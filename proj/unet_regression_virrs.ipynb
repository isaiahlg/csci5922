{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Earth at Night in HD: Predicting High-Resolutions Nighttime Radiance from Daytime Satellite Imagery\n",
        "Isaiah Lyons-Galante  \n",
        "University of Colorado Boulder   \n",
        "CSCI 5922 Neural Networks and Deep Learning  \n",
        "Final Project  \n",
        "Fall 2023  \n",
        "\n",
        "![earth at night](./figs/earthatnight.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The imprint of human activity on Earth has become unmistakable, offering a unique vantage point for population studies through the lens of space. Access to global high-resolution datasets, such as Landsat(1), has been made possible for the public through platforms like Google Earth Engine (3). These datasets open a window to observe and analyze human settlements, providing invaluable insights into the dynamics of our changing world. The image below is from Google Earth, showing the twin cities of Brazzaville and Kishasa on the banks of the Congo River. One can clearly see the densely package settlements on the banks, along with the unpopulated islands in the river.\n",
        "\n",
        "![brazzaville](./figs/brazzaville.png)\n",
        "\n",
        "During the night, human presence becomes even more apparent as settlements cast a faint glow into space. This nocturnal illumination is captured by the VIIRS instrument aboard the Suomi National Polar-orbiting Partnership (NPP) satellite. Despite its significance, the relatively weak signals limit the resolution to about 500 meters squared. This limitation poses a challenge to extracting fine-grained information about the distribution and characteristics of human settlements. The image below is of the same area as the one above, but at night. The resolution is significantly lower, with the pixels clearly visible, while the island in the river is completely dark.\n",
        "\n",
        "![brazzaville night](./figs/brazzavilleatnight.png)\n",
        "\n",
        "Compelling research underscores the correlation between night lights and mean household wealth, as measured by ground surveys. This correlation establishes night lights as a proxy for gauging livelihoods, making them a valuable tool for socio-economic analyses. This becomes particularly crucial in regions like Sub-Saharan Africa, where traditional population surveys are sparse. The scarcity of on-the-ground data highlights the potential of night lights as a surrogate measure for understanding the economic landscapes of communities. The graph below shows the relationship between night lights and mean household wealth in Sub-Saharan Africa. The data is from the Demographic and Health Surveys (DHS) Program, which conducts household surveys in developing countries. The graph shows a clear correlation between night lights and mean household wealth.\n",
        "\n",
        "![night lights and wealth](./figs/wealthcorrelations.jpg)\n",
        "\n",
        "The demand for higher-resolution night lights becomes evident when considering the need for detailed impact assessments. Such assessments can unravel the intricate ways in which individual communities are affected by economic development programs or global events. As technology advances, the quest for sharper and more detailed night light data gains significance, promising to enhance our understanding of human settlements and socio-economic patterns on a global scale. One example of this comes from a paper in Nature just last year that use wealth maps to assess the economic impact of access to electricity. High resolution nightlights would open up even finer grained analysis.\n",
        "\n",
        "![wealth map](./figs/WealthMap.png)\n",
        "\n",
        "Embarking on this exploration of human settlements and socio-economic patterns, I employ cutting-edge methodologies to surmount the resolution challenges posed by existing night light data. Leveraging the power of machine learning, my approach involves training a model to predict night lights from high-resolution daytime imagery. By undertaking this innovative process, I aim to transcend the limitations of the existing data and generate high-resolution nighttime imagery. This fusion of technological advancements with geospatial analysis not only promises to enrich our understanding of human settlements but also sets the stage for a more nuanced examination of the impact of economic development programs and global events on individual communities. This notebook will serve as a guide for the entire process, from method design to model training and evaluation, and finally, the results and conclusions. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Methods\n",
        "\n",
        "Goal: \n",
        "\n",
        "\n",
        "\n",
        "\n",
        " output is VIIRS Day/Night Band from [VIIRS](https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_DNB_MONTHLY_V1_VCMSLCFG) and the input is a Landsat 8 composite.  The model is a [fully convolutional neural network (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597). In this notebook, we:\n",
        "\n",
        "1.   Export train/test patches from Earth Engine, suitable for training an FCNN model.\n",
        "2.   Preprocess the data. \n",
        "3.   Train and validating an FCNN model.\n",
        "4.   Maki predictions with the trained model and import them to Earth Engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Authenticate and import as necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('GCS_PROJECT', 'master-thesis-ilg'), ('BUCKET', 'csci5922-proj'), ('HOME_PATH', '/home/isly9493/'), ('LOCAL_PATH', '/local/isly9493/'), ('DATA_PATH', 'csci5922/proj/data/'), ('FOLDER', 'viirs-africa-many/'), ('GCLOUD_PATH', '/home/isly9493/google-cloud-sdk/bin/'), ('CUDA_VISIBLE_DEVICES', '1')])\n"
          ]
        }
      ],
      "source": [
        "# Read in the .env file\n",
        "from dotenv import dotenv_values, load_dotenv\n",
        "\n",
        "# load env variables\n",
        "config = dotenv_values(\".env\")\n",
        "load_dotenv()\n",
        "\n",
        "# set env variables\n",
        "GCS_PROJECT = config['GCS_PROJECT']\n",
        "BUCKET = config['BUCKET']\n",
        "HOME_PATH = config['HOME_PATH']\n",
        "LOCAL_PATH = config['LOCAL_PATH']\n",
        "DATA_PATH = config['DATA_PATH']\n",
        "FOLDER = config['FOLDER']\n",
        "GCLOUD_PATH = config['GCLOUD_PATH']\n",
        "CUDA_VISIBLE_DEVICES = config['CUDA_VISIBLE_DEVICES']\n",
        "\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jat01FEoUMqg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n",
            "0.15.1\n"
          ]
        }
      ],
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# Folium setup.\n",
        "import folium\n",
        "print(folium.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8ycmzClYwf"
      },
      "source": [
        "# Variables\n",
        "\n",
        "Declare the variables that will be in use throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKs6HuxOzjMl"
      },
      "source": [
        "## Specify your Cloud Storage Bucket\n",
        "You must have write access to a bucket to run this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "## Set other global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "psz7wJKalaoj"
      },
      "outputs": [],
      "source": [
        "# variables for image names\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'avg_rad' ##### \n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 4800 # orignally 16000, use 800 for small version, 8000 for medium version\n",
        "EVAL_SIZE = 200 # originally 8000, use 200 for small version\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 48 # orignally 16\n",
        "EPOCHS = 20 # originally 10\n",
        "BUFFER_SIZE = 2000 # originally 2000\n",
        "OPTIMIZER = 'SGD'\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Create and Stack Input + Target Imagery\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 8 composite.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_6e104d8372934bcdb7fe02b76105b930 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_6e104d8372934bcdb7fe02b76105b930&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_6e104d8372934bcdb7fe02b76105b930 = L.map(\n",
              "                &quot;map_6e104d8372934bcdb7fe02b76105b930&quot;,\n",
              "                {\n",
              "                    center: [-1.0, 37.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 6,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_e2f09b1f4323d35257d954b0320412ca = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_e2f09b1f4323d35257d954b0320412ca.addTo(map_6e104d8372934bcdb7fe02b76105b930);\n",
              "        \n",
              "    \n",
              "            var tile_layer_477223ab3362eedc39d81a16d215e7c1 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/310f3b2426b7bf7f16371db8e9f1c0fa-4bd87bd28a06252d0b6420221dfb3d11/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_477223ab3362eedc39d81a16d215e7c1.addTo(map_6e104d8372934bcdb7fe02b76105b930);\n",
              "        \n",
              "    \n",
              "            var tile_layer_79b468c8becdcaab27846ab043bb4220 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/7a203266941255964bbbf00c04322f82-a16f4e02cebf9d9bfed96a66fa9f684c/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_79b468c8becdcaab27846ab043bb4220.addTo(map_6e104d8372934bcdb7fe02b76105b930);\n",
              "        \n",
              "    \n",
              "            var layer_control_6785bace62067ef6bdf2c2e2b69562c7_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_e2f09b1f4323d35257d954b0320412ca,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;median composite&quot; : tile_layer_477223ab3362eedc39d81a16d215e7c1,\n",
              "                    &quot;thermal&quot; : tile_layer_79b468c8becdcaab27846ab043bb4220,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_6785bace62067ef6bdf2c2e2b69562c7 = L.control.layers(\n",
              "                layer_control_6785bace62067ef6bdf2c2e2b69562c7_layers.base_layers,\n",
              "                layer_control_6785bace62067ef6bdf2c2e2b69562c7_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_6e104d8372934bcdb7fe02b76105b930);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f854a8bf650>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "image = l8sr.filterDate('2020-01-01', '2022-12-31').map(maskL8sr).median()\n",
        "\n",
        "# printn out the bands\n",
        "print(image.bandNames().getInfo())\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = image.getMapId({'bands': ['B10'], 'min': 0, 'max': 0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='thermal',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ"
      },
      "source": [
        "Prepare the response (what we want to predict).  This is nighttime brightness (in nanoWatts/sr/cm^2) from the VIIRS dataset.  Display to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e0wHDyxVirec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_82c66a6dc6e5ff040a3cd72357ec5e95 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_82c66a6dc6e5ff040a3cd72357ec5e95&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_82c66a6dc6e5ff040a3cd72357ec5e95 = L.map(\n",
              "                &quot;map_82c66a6dc6e5ff040a3cd72357ec5e95&quot;,\n",
              "                {\n",
              "                    center: [-1.0, 37.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 6,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_1d7615338b3696657be4887f9198b9f8 = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_1d7615338b3696657be4887f9198b9f8.addTo(map_82c66a6dc6e5ff040a3cd72357ec5e95);\n",
              "        \n",
              "    \n",
              "            var tile_layer_c9162c6467f65a3868b43db9d4ae2bdb = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/cd3a34ef7b3cb61d05ee334012d7a307-3c455a4969a5648c8734258fce463d6d/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_c9162c6467f65a3868b43db9d4ae2bdb.addTo(map_82c66a6dc6e5ff040a3cd72357ec5e95);\n",
              "        \n",
              "    \n",
              "            var layer_control_9700ae15f2be1f2e1551f10c3eda68d3_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_1d7615338b3696657be4887f9198b9f8,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;viirs dnb&quot; : tile_layer_c9162c6467f65a3868b43db9d4ae2bdb,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_9700ae15f2be1f2e1551f10c3eda68d3 = L.control.layers(\n",
              "                layer_control_9700ae15f2be1f2e1551f10c3eda68d3_layers.base_layers,\n",
              "                layer_control_9700ae15f2be1f2e1551f10c3eda68d3_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_82c66a6dc6e5ff040a3cd72357ec5e95);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f8549f5f5d0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2020-01-01', '2022-12-31').median()\n",
        "viirs = viirs.divide(2).float() # normalize to 0-1 range\n",
        "\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) # normally max 1.0\n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Stack the 2D images (Landsat composite and VIIRS DNB image) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eGHYsdAOipa4"
      },
      "outputs": [],
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  viirs.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "# Define Training and Evaluation Areas\n",
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ure_WaD0itQY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_ff8c0ff5bda23e8e9a5fc67224b53cce {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_ff8c0ff5bda23e8e9a5fc67224b53cce&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_ff8c0ff5bda23e8e9a5fc67224b53cce = L.map(\n",
              "                &quot;map_ff8c0ff5bda23e8e9a5fc67224b53cce&quot;,\n",
              "                {\n",
              "                    center: [0.0, 12.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 5,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_58f87ba566b70a079cb627048407805c = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_58f87ba566b70a079cb627048407805c.addTo(map_ff8c0ff5bda23e8e9a5fc67224b53cce);\n",
              "        \n",
              "    \n",
              "            var tile_layer_dc21957ce633d69fe1bc4138281e8599 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/89f13b8e474972f1cb143a8ae12ab0dc-d1cd2cd11bdc790c5db5de792728bb2a/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_dc21957ce633d69fe1bc4138281e8599.addTo(map_ff8c0ff5bda23e8e9a5fc67224b53cce);\n",
              "        \n",
              "    \n",
              "            var layer_control_ae3fc081879b1cc08d4e88a3612f75e0_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_58f87ba566b70a079cb627048407805c,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;training polygons&quot; : tile_layer_dc21957ce633d69fe1bc4138281e8599,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_ae3fc081879b1cc08d4e88a3612f75e0 = L.control.layers(\n",
              "                layer_control_ae3fc081879b1cc08d4e88a3612f75e0_layers.base_layers,\n",
              "                layer_control_ae3fc081879b1cc08d4e88a3612f75e0_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_ff8c0ff5bda23e8e9a5fc67224b53cce);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f854ac64390>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# trainingPolys = ee.FeatureCollection('projects/google/DemoTrainingGeometries')\n",
        "# evalPolys = ee.FeatureCollection('projects/google/DemoEvalGeometries')\n",
        "# # trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolys')\n",
        "# # evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolys')\n",
        "# trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolysAfrica')\n",
        "# evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolysAfrica')\n",
        "trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolysAfricaBig')\n",
        "evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolysAfrica')\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['gold', 'blue']})\n",
        "map = folium.Map(location=[0, 12], zoom_start=5)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do NOT need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A"
      },
      "outputs": [],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "evalPolysList = evalPolys.toList(evalPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 20 # Number of shards in each polygon. # originally 200\n",
        "N = 200 # Total sample size in each polygon. # orignally 2000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export Training & Evaluation Data from Earth Engine to Google Cloud Storage\n",
        "This can be modified to export to local storage, just adjust the file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 viirs-africa-many/ csci5922-proj training_patches_g0\n",
            "1 viirs-africa-many/ csci5922-proj training_patches_g1\n",
            "2 viirs-africa-many/ csci5922-proj training_patches_g2\n",
            "3 viirs-africa-many/ csci5922-proj training_patches_g3\n",
            "4 viirs-africa-many/ csci5922-proj training_patches_g4\n",
            "5 viirs-africa-many/ csci5922-proj training_patches_g5\n",
            "6 viirs-africa-many/ csci5922-proj training_patches_g6\n",
            "7 viirs-africa-many/ csci5922-proj training_patches_g7\n",
            "8 viirs-africa-many/ csci5922-proj training_patches_g8\n",
            "9 viirs-africa-many/ csci5922-proj training_patches_g9\n",
            "10 viirs-africa-many/ csci5922-proj training_patches_g10\n",
            "11 viirs-africa-many/ csci5922-proj training_patches_g11\n",
            "12 viirs-africa-many/ csci5922-proj training_patches_g12\n",
            "13 viirs-africa-many/ csci5922-proj training_patches_g13\n",
            "14 viirs-africa-many/ csci5922-proj training_patches_g14\n",
            "15 viirs-africa-many/ csci5922-proj training_patches_g15\n",
            "16 viirs-africa-many/ csci5922-proj training_patches_g16\n",
            "17 viirs-africa-many/ csci5922-proj training_patches_g17\n",
            "18 viirs-africa-many/ csci5922-proj training_patches_g18\n",
            "19 viirs-africa-many/ csci5922-proj training_patches_g19\n",
            "20 viirs-africa-many/ csci5922-proj training_patches_g20\n",
            "21 viirs-africa-many/ csci5922-proj training_patches_g21\n",
            "22 viirs-africa-many/ csci5922-proj training_patches_g22\n",
            "23 viirs-africa-many/ csci5922-proj training_patches_g23\n",
            "Training imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Export all the training data (in many pieces), with one task\n",
        "# per geometry.\n",
        "for g in range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "  \n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Training imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 viirs-africa-many/ csci5922-proj eval_patches_g0\n",
            "1 viirs-africa-many/ csci5922-proj eval_patches_g1\n",
            "Eval imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Eval imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy Files from Google Cloud Storage to Local Storage\n",
        "This is necessary to run the model locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Copy from your bucket to local path (note -r is for recursive call)\n",
        "# !{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{TRAINING_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "# !{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{EVAL_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "\n",
        "!{GCLOUD_PATH}gsutil cp 'gs://csci5922-proj/viirs-africa-big/training_patches_g23.tfrecord.gz' {LOCAL_PATH}{DATA_PATH}{FOLDER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure GPU Usage\n",
        "\n",
        "Depending on how many GPUs are needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see what devices are available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
        "!nvidia-smi\n",
        "# tf.debugging.set_log_device_placement(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configure the strategy to use all GPUs\n",
        "# gpus = tf.config.list_logical_devices('GPU')\n",
        "# gpu_strategy = tf.distribute.MirroredStrategy(gpus)\n",
        "\n",
        "# gpu_strategy = tf.device('/GPU:1')\n",
        "\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Training and Evaluation Data\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "outputs": [],
      "source": [
        "# from google.cloud import storage\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(path, prefix, n_files):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  # original code\n",
        "  # glob = tf.io.gfile.glob(pattern)\n",
        "  \n",
        "  # from the cloud\n",
        "  # https://cloud.google.com/appengine/docs/legacy/standard/python/googlecloudstorageclient/read-write-to-cloud-storage\n",
        "  # storage_client = storage.Client()\n",
        "  # bucket = storage_client.get_bucket(BUCKET)\n",
        "  # blobs = storage_client.list_blobs(BUCKET, prefix=pattern)\n",
        "  # glob = []\n",
        "  # for blob in blobs:\n",
        "  #   globs.append(blob.name)\n",
        "  # print(globs)\n",
        "  # # append bucket name to globs\n",
        "  # glob = ['gs://' + BUCKET + '/' + f for f in globs]\n",
        "\n",
        "  # read local tfrecord.gz files from unet-mini\n",
        "  glob = []\n",
        "  for i in range(0, n_files):\n",
        "    glob = path + prefix + '_g' + str(i) + '.tfrecord.gz'\n",
        "\n",
        "  # glob = [\n",
        "  #   '/home/isly9493/csci5922/proj/data/unet-mini/eval_patches_g0.tfrecord.gz',\n",
        "  #   '/home/isly9493/csci5922/proj/data/unet-mini/eval_patches_g1.tfrecord.gz'\n",
        "  # ]\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "Use the helpers to read in the training dataset.  Print the first record to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0qRF0fAYcC"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/GPU:1'):\n",
        "# with gpu_strategy.scope():\n",
        "training = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, TRAINING_BASE, 24)\n",
        "training = training.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# print(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "# Evaluation data\n",
        "\n",
        "Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, the evaluation dataset has a batch size of 1, is not repeated and is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKTCGiJ6xzo"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/GPU:1')\n",
        "# with gpu_strategy.scope():\n",
        "evaluation = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, EVAL_BASE, 2)\n",
        "evaluation = evaluation.batch(1).repeat()\n",
        "# print(iter(evaluation.take(1)).next()[0][0,:,:,1:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize of the of the training / evaluation image patch pairs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# # Display a batch of data\n",
        "def display_batch_of_images(databatch):\n",
        "    \"\"\"Display a batch of images.\n",
        "    Args:\n",
        "        databatch: A batch of data\n",
        "    \"\"\"\n",
        "    imgs, labels = databatch\n",
        "    \n",
        "    # show landsat in RGD with blue as band 2, green as band 3, and red as band 4\n",
        "    plt.figure(figsize=(10,10))\n",
        "    label = labels.numpy().astype(np.float32)\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(imgs[i,:,:,2:5]*6)\n",
        "        # plt.imshow(tf.concat([\n",
        "        #     imgs[i,:,:,4],\n",
        "        #     imgs[i,:,:,3],\n",
        "        #     imgs[i,:,:,2]\n",
        "        # ]))\n",
        "        # plt.title('LandSat 8')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    # show viirs in grayscale\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(labels[i,:,:,0], cmap='gray')\n",
        "        # plt.title('VIIRS DNB')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display a batch of data\n",
        "display_batch_of_images(iter(training.take(1)).next())\n",
        "# display_batch_of_images(iter(evaluation.take(1)).next())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Model\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model.  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem.  Since impervious surface fraction is constrained to [0,1], with many values close to zero or one, a saturating activation function is suitable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import models\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\t\n",
        "\t# set optimizer with custom learning rate\n",
        "\tOPTIMIZER = 'SGD'\n",
        "\toptimizer = optimizers.get(OPTIMIZER)\n",
        "\t# optimizer.learning_rate = 0.1\n",
        "\n",
        "\t# compile the model\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizer,\n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "outputs": [],
      "source": [
        "# print the current time\n",
        "from datetime import datetime\n",
        "print(datetime.now())\n",
        "\n",
        "# with tf.device('/GPU:1')\n",
        "# with gpu_strategy.scope():\n",
        "m = get_model()\n",
        "history = m.fit(\n",
        "    x=training,\n",
        "    epochs=20,\n",
        "    # steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE),\n",
        "    steps_per_epoch=1000,\n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE\n",
        ")\n",
        "\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = 'viirs_africa_big_e20_tiny_batch.keras'\n",
        "HISTORY_NAME = 'viirs_africa_big_e20_tiny_batch_history.pickle'\n",
        "ASSET_SUFFIX = 'viirs_africa_big_e20_tiny_batch' \n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model to a local file \n",
        "m.save(LOCAL_PATH + DATA_PATH + FOLDER + MODEL_NAME)\n",
        "# m.save(MODEL_NAME)\n",
        "\n",
        "# # save the history variable to a local file with pickle\n",
        "import pickle\n",
        "with open(LOCAL_PATH + DATA_PATH + FOLDER + HISTORY_NAME, 'wb') as f:\n",
        "# with open(HISTORY_NAME, 'wb') as f:\n",
        "  pickle.dump(history, f)\n",
        "# issue: doesn't save the history field of the history object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XrwZHp66j4"
      },
      "source": [
        "Note that the notebook VM is sometimes not heavy-duty enough to get through a whole training job, especially if you have a large buffer size or a large number of epochs.  You can still use this notebook for training, but may need to set up an alternative VM ([learn more](https://research.google.com/colaboratory/local-runtimes.html)) for production use.  Alternatively, you can package your code for running large training jobs on Google's AI Platform [as described here](https://cloud.google.com/ml-engine/docs/tensorflow/trainer-considerations).  The following code loads a pre-trained model, which you can use for predictions right away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-RJpNfEUS1qp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-12 14:27:30.350520: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.372188: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.372377: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.377419: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.377570: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.377712: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.437613: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.437761: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.437884: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-12 14:27:30.437984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46855 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:9c:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "# MODEL_DIR = 'gs://ee-docs-demos/fcnn-demo/trainer/model'\n",
        "# m = tf.keras.models.load_model('/local/isly9493/csci5922/proj/data/viirs-africa/unet_viirs_e50.keras')\n",
        "# m = tf.keras.models.load_model(MODEL_NAME)\n",
        "# m.summary()\n",
        "\n",
        "# # load the history variable from a local file with pickle\n",
        "import pickle\n",
        "# with open(HISTORY_NAME, 'rb') as f:\n",
        "with open('/local/isly9493/csci5922/proj/data/viirs-africa-big/viirs_africa_big_e20_tiny_batch_history.pickle', 'rb') as f:\n",
        "  history = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "Let's look at how the model performed in terms of loss while it trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['root_mean_squared_error'])\n",
        "plt.plot(history.history['val_root_mean_squared_error'])\n",
        "plt.title('Model RMSE for ' + ASSET_SUFFIX + ' for ' + str(EPOCHS) + ' epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training RMSE', 'Validation RMSE'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## evaluate the model on the evaluation dataset to get the loss and RMSE\n",
        "loss, rmse = m.evaluate(evaluation, steps=EVAL_SIZE)\n",
        "print('Model loss: {:0.4f}. RMSE: {:0.4f}'.format(loss, rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, rmse = m.evaluate(training, steps=10)\n",
        "print('Model loss: {:0.4f}. RMSE: {:0.4f}'.format(loss, rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storage bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n",
        "\n",
        "The following functions handle this process.  It's useful to separate the export from the predictions so that you can experiment with different models without running the export every time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specify Region and Outputs for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPANwc7B1-TS"
      },
      "outputs": [],
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "ee_user_folder = 'projects/'+GCS_PROJECT+'/assets' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# # Base file name to use for TFRecord files and assets.\n",
        "# pred_image_base = 'FCNN_demo_beijing_384_'\n",
        "# # Half this will extend on the sides of each patch.\n",
        "# pred_kernel_buffer = [128, 128]\n",
        "# # Beijing\n",
        "# pred_region = ee.Geometry.Polygon(\n",
        "#         [[[115.9662455210937, 40.121362012835235],\n",
        "#           [115.9662455210937, 39.64293313749715],\n",
        "#           [117.01818643906245, 39.64293313749715],\n",
        "#           [117.01818643906245, 40.121362012835235]]], None, False)\n",
        "\n",
        "# kinshasa center: 15.267681, -4.286977\n",
        "pred_image_base = 'FCNN_demo_kinshasa_384_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "pred_kernel_buffer = [128, 128]\n",
        "# Kinshasa\n",
        "pred_region = ee.Geometry.Polygon(\n",
        "  [[[15.02, -4.04],\n",
        "    [15.02, -4.54],\n",
        "    [15.52, -4.54],\n",
        "    [15.52, -4.04]]], \n",
        "  None, \n",
        "  False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Sample Imagery for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + out_image_base,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "  print('Prediction image export task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')\n",
        "\n",
        "  # Block until the task completes.\n",
        "  # import time\n",
        "  # while task.active():\n",
        "  #   time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  # if task.status()['state'] != 'COMPLETED':\n",
        "  #   print('Error with image export.')\n",
        "  # else:\n",
        "  #   print('Image export completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "outputs": [],
      "source": [
        "# Run the export.\n",
        "doExport(pred_image_base, pred_kernel_buffer, pred_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy the file from cloud storage to local\n",
        "# !{GCLOUD_PATH}gsutil -m cp \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00000.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00001.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00002.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00003.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00004.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'mixer.json' \\\n",
        "#   {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "\n",
        "# !{GCLOUD_PATH}gsutil -m cp -r 'gs://csci5922-proj/viirs-africa/FCNN_demo_kinshasa*' 'gs://csci5922-proj/viirs-africa-small/'\n",
        "\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Code for Running Model on Prediction Data\n",
        "Modified to run on local data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_9_FflygVw"
      },
      "outputs": [],
      "source": [
        "def doPrediction(out_image_base, kernel_buffer):\n",
        "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # # Get a list of all the files in the output bucket.\n",
        "  # filesList = !gsutil ls 'gs://'{BUCKET}'/'{FOLDER}\n",
        "  # Get a list of all local files in /data/unet-mini\n",
        "  filesList = !ls {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print('here I am')\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  # jsonText = !{GCLOUD_PATH}gsutil cat {jsonFile}  \n",
        "  # jsonText = !cat '/home/isly9493/csci5922/proj/data/viirs-usa/FCNN_demo_beijing_384_mixer.json'\n",
        "  jsonText = !cat {LOCAL_PATH}{DATA_PATH}{FOLDER}{jsonFile}\n",
        "  print(jsonText)\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "  # append absolute path to imageFilesList\n",
        "  imageFilesList = [LOCAL_PATH + DATA_PATH + FOLDER + f for f in imageFilesList]\n",
        "  print(imageFilesList)\n",
        "  \n",
        "  # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # print(imageDataset)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def writePredictions(predictions, out_image_base, kernel_buffer):\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "  # out_image_file = LOCAL_PATH + DATA_PATH + FOLDER + out_image_base + '.TFRecord'\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "  \n",
        "  \n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    # print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'impervious': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(\n",
        "              value=predictionPatch.flatten()\n",
        "            )\n",
        "          )\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  print('Done writing predictions.')\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uploadPredictionsToEE(out_image_base, ee_user_folder):\n",
        "  # Start the upload.\n",
        "  print('Uploading to Google Earth Egnine...')\n",
        "  \n",
        "  # define file paths\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "  out_image_asset = ee_user_folder + '/' + out_image_base + ASSET_SUFFIX\n",
        "  jsonFile = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + 'mixer.json'\n",
        "  # jsonFile = '/home/isly9493/csci5922/proj/data/unet-mini/FCNN_demo_beijing_384_mixer.json'\n",
        "  \n",
        "  print('out_image_asset: ', out_image_asset)\n",
        "  print('out_image_file: ', out_image_file)\n",
        "  print('jsonFile: ', jsonFile)\n",
        "  \n",
        "  # upload file to google earth engine from gcs\n",
        "  # !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "\n",
        "  print('Prediction image upload task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy over prediction files\n",
        "# !{GCLOUD_PATH}gsutil -m cp \\\n",
        "#   \"gs://csci5922-proj/viirs-africa-small/FCNN_demo_kinshasa_384_.TFRecord\" \\\n",
        "#   \"gs://csci5922-proj/viirs-africa-small/FCNN_demo_kinshasa_384_00000.tfrecord.gz\" \\\n",
        "#   \"gs://csci5922-proj/viirs-africa-small/FCNN_demo_kinshasa_384_00001.tfrecord.gz\" \\\n",
        "#   \"gs://csci5922-proj/viirs-africa-small/FCNN_demo_kinshasa_384_00002.tfrecord.gz\" \\\n",
        "#   \"gs://csci5922-proj/viirs-africa-small/FCNN_demo_kinshasa_384_mixer.json\" \\\n",
        "#   \"gs://csci5922-proj/viirs-africa-big/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqlymOehnQO"
      },
      "source": [
        "Now there's all the code needed to run the prediction pipeline, all that remains is to specify the output region in which to do the prediction, the names of the output files, where to put them, and the shape of the outputs.  In terms of the shape, the model is trained on 256x256 patches, but can work (in theory) on any patch that's big enough with even dimensions ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)).  Because of tile boundary artifacts, give the model slightly larger patches for prediction, then clip out the middle 256x256 patch.  This is controlled with a kernel buffer, half the size of which will extend beyond the kernel buffer.  For example, specifying a 128x128 kernel will append 64 pixels on each side of the patch, to ensure that the pixels in the output are taken from inputs completely covered by the kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "outputs": [],
      "source": [
        "# Run the prediction.\n",
        "predictions = doPrediction(pred_image_base, pred_kernel_buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the predictions.\n",
        "writePredictions(predictions, pred_image_base, pred_kernel_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload to earth engine\n",
        "uploadPredictionsToEE(pred_image_base, ee_user_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "## Display the Prediction\n",
        "\n",
        "One the data has been exported, the model has made predictions and the predictions have been written to a file, and the image imported to Earth Engine, it's possible to display the resultant Earth Engine asset.  Here, display the impervious area predictions over Beijing, China."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgco6HJ4R5p2"
      },
      "outputs": [],
      "source": [
        "map = folium.Map(location=[-4.29, 15.27], zoom_start=11)\n",
        "\n",
        "\n",
        "# add landsat layer clipped to prediction region\n",
        "landsat = image\n",
        "landsat = landsat.clip(pred_region)\n",
        "mapid = landsat.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='landsat',\n",
        "  ).add_to(map)\n",
        "\n",
        "# actual viirs\n",
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2021-01-01', '2022-12-31').median().divide(30).float()\n",
        "viirs = viirs.clip(pred_region)\n",
        "# actual_image = viirs\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) # normally max 1.0\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "\n",
        "# prediction\n",
        "out_image = ee.Image(ee_user_folder + '/' + pred_image_base + ASSET_SUFFIX)\n",
        "mapid = out_image.getMapId({'min': 0, 'max': 1})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted nightlights',\n",
        "  ).add_to(map)\n",
        "\n",
        "\n",
        "# difference\n",
        "diff = out_image.subtract(viirs)\n",
        "mapid = diff.getMapId({'min': -0.5, 'max': 0.5, 'palette': ['FF0000', '000000', '00FF00']})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='difference',\n",
        "  ).add_to(map)\n",
        "\n",
        "# visualize the map\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "The top performing model was the one trained on the small polygons around African cities for 20 epochs using Stochastic Gradient Descent, achieving an RMSE of 1.504. Surprisingly, increasing the samples in the training dataset by 10X did not improve on the results. Another surprise is that the train RMSE was consistently lower than the validation RMSE. This is highly atypical, and suggests that there is an issue with the training strategy. The full table of results is below.\n",
        "\n",
        "![results](./figs/results.png)\n",
        "\n",
        "On these RMSE by epoch graphs, we have a few interesting results. (1) shows the model training and validating in the USA, the only one where validation error followed a more regular trend. (2) shows the training with the Adam Optimizer, showing quite unstable training. (3) shows the stable training of one of the top performing models. However, it reaches quite close to the minimum in just about 15 epochs, and does not improve from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions\n",
        "\n",
        "In the pursuit of enhancing the accuracy and applicability of our night light prediction model, several key observations and avenues for future exploration have emerged. One notable finding revolves around the impact of training data size on model performance. Contrary to expectations, an increase in the size of the training data did not yield the anticipated improvement. This unexpected outcome prompts a deeper investigation into the relationship between training sample size, model parameters, batch size, learning rate, and the number of epochs. Unraveling this intricate interplay could offer valuable insights and potentially establish guidelines for optimizing model training.\n",
        "\n",
        "A second area of consideration is the resolution of the imagery used in the model. Although the results did not meet initial expectations, the opportunity to explore even higher resolution imagery, such as Sentinel-2 or Google Earth base maps, remains untapped. The potential gains from employing more detailed data sources could contribute significantly to refining the accuracy of our predictions, warranting further experimentation and analysis.\n",
        "\n",
        "Another noteworthy aspect pertains to the optimal number of epochs in training the model. Observations indicate that most models plateau well before reaching 50 epochs, suggesting a potential saturation point. This phenomenon is likely intertwined with the challenges posed by the training data size. Investigating the intricate dynamics among training data, model parameters, and epoch count is essential to discerning the optimal configuration for robust performance.\n",
        "\n",
        "Looking ahead, the goal extends beyond night light prediction alone. Once a model with satisfactory accuracy is achieved, the intention is to fine-tune it using ground surveys of household assets. This strategic refinement aims to enable a nuanced assessment of economic status through the lens of predicted night lights, reinforcing the socio-economic dimension of the research.\n",
        "\n",
        "As we chart the course for future work, a spectrum of possibilities emerges for both improving the current model's performance and exploring alternative models. Future endeavors may involve incorporating rural areas into the training and evaluation sets, exploring the impact of reducing samples per patch, and experimenting with image patch manipulations such as rotation and mirroring. Moreover, considerations extend to the redundancy in the current training data, prompting potential improvements through techniques like leave-one-out cross-validation and a closer examination of label distributions.\n",
        "\n",
        "Looking beyond incremental enhancements, future models could benefit from substituting Landsat 8 with Sentinel 2, reevaluating with Planet imagery, or exploring the application of Vision Transformers (ViTs). Additionally, the scope extends to examining wealth data rather than night lights, and investigating the influence of dataset size on model performance. These future directions collectively pave the way for a comprehensive and evolving exploration of the intersection between machine learning, satellite imagery, and socio-economic analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n",
        "\n",
        "1. Landsat 8: https://www.usgs.gov/landsat-missions/landsat-8 \n",
        "1. Google Earth Engine: https://earthengine.google.com/\n",
        "1. Suomi NPP VIIRS Instrument: https://ncc.nesdis.noaa.gov/VIIRS/\n",
        "1. Using night light emissions for the prediction of local wealth: https://journals.sagepub.com/doi/full/10.1177/0022343316630359\n",
        "1. Using publicly available satellite imagery and deep learning to understand economic well-being in Africa: https://www.nature.com/articles/s41467-020-16185-w\n",
        "1. Fully Convolutional Networks for Semantic Segmentation: https://arxiv.org/abs/1411.4038\n",
        "1. U-Net: Convolutional Networks for Biomedical Image Segmentation: https://arxiv.org/abs/1505.04597\n",
        "1. VIIRS DNB Monthly Stray-Light Corrected Composite: https://eogdata.mines.edu/products/vnl/#monthly\n",
        "1. GEE Python API: https://developers.google.com/earth-engine/tutorials/community/intro-to-python-api\n",
        "1. Tensorflow: https://www.tensorflow.org/about/bib \n",
        "1. GitHub Code Repo: https://github.com/isaiahlg/unet-viirs\n",
        "1. ESA’s Copernicus Sentinel-2: https://sentinel.esa.int/web/sentinel/missions/sentinel-2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix A: Export to HTML\n",
        "\n",
        "Below is a little code snippet that turns this Jupyter notebook into an HTML file to use as a webpage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export to HTML for webpage\n",
        "import os\n",
        "\n",
        "# os.system('jupyter nbconvert --to html mod1.ipynb')\n",
        "os.system('jupyter nbconvert --to html unet_regression_virrs.ipynb --HTMLExporter.theme=dark')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix B: To Do List\n",
        "This is a list I used to guide my work on this project, along with some future ideas for my own future reference!\n",
        "\n",
        "Decide a model to Use\n",
        "- [x] Figure out how to extract image patches for model training\n",
        "- [x] Extract Sentinel 2 and Planet Data\n",
        "- [x] Figure out which type of model to use\n",
        "- [x] Figure out how to run one of these scripts  \n",
        "\n",
        "Get a Simple Notebook Running\n",
        "- [x] Get the notebook running locally with the tfrecords saved locally  \n",
        "- [x] Train a model with small subset of the data  \n",
        "- [x] Get the model prediction and inference code to run with local files\n",
        "- [x] Get a simple version of the model trained\n",
        "- [x] Substitute NLCD with nightlights  \n",
        "- [x] Retrain a simple model with the new data  \n",
        "\n",
        "Get the Model Running on the Lab Computer\n",
        "- [x] Set up a python env on the lab computer  \n",
        "- [x] Set up SSH on the lab computer\n",
        "- [x] Adapt script to run on lab computer\n",
        "- [x] Run on lab computer  \n",
        "\n",
        "Scale Up the Model \n",
        "- [x] Visualize the model accuracy by epoch\n",
        "- [x] Compare a prediction to the actual image\n",
        "- [x] Create a difference image\n",
        "- [x] Scale up the model with original image patches\n",
        "\n",
        "Figure out why the model didn't train well..\n",
        "- [x] Resetup model to run with the small image patches - got to RMSE 0.16\n",
        "- [x] Try to set up with image patches in SSA - won't learn :/\n",
        "- [x] Try adjusting the optimizer\n",
        "- [x] Try with smaller image patches in SSA just arond cities\n",
        "- [x] Update Africa large to evaluate on small image patches\n",
        "- [x] Compare performance of large and small model\n",
        "- [ ] Try tripling the number of cities\n",
        "\n",
        "Generate Imagery and Figures:\n",
        "- [x] Run predictions on the validation sets as well\n",
        "- [x] Create a standardized test set\n",
        "- [x] Rvaluate the large model on the small patches to compare performance\n",
        "\n",
        "Future Ideas for Improving Model Performance\n",
        "- [ ] Try adding in rural areas for train and eval\n",
        "- [ ] Get fewer samples per patch to see if it still performs as well\n",
        "- [ ] Consider rotating and mirroring the image patches, easy in Pytorch\n",
        "- [ ] Probably a lot of redundancy right now in training data\n",
        "- [ ] Try leave one out cross validation\n",
        "- [ ] Plot distributions of labels\n",
        " \n",
        "\n",
        "Future Ideas for New Models:\n",
        "- [ ] Substitute Landsat 8 with Sentinel 2  \n",
        "- [ ] Consider running again with Planet  \n",
        "- [ ] Consider running again with a ViTs\n",
        "- [ ] Consider running again with \n",
        "- [ ] Look wealth instead of nightlights data  \n",
        "- [ ] Look at effect of dataset size on model performance\n",
        "\n",
        "Parameters Tested Out:\n",
        "- [ ] Number of epochs\n",
        "- [ ] Batch size\n",
        "- [ ] Learning rate\n",
        "- [ ] Optimizer\n",
        "- [ ] Loss function\n",
        "- [ ] Model architecture\n",
        "- [ ] Image patch size\n",
        "- [ ] Number of image patches per city\n",
        "- [ ] Number of cities\n",
        "- [ ] Location of cities"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
